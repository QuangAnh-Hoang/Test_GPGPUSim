# -*- coding: utf-8 -*-
"""LeNet-5_Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14OO613u8ST7tjXKUZfGor0Esd6iTl-0w
"""

! pip install pytorch-lightning==0.8.3

import os

import torch
from torch.nn import functional as F
from torch.utils.data import DataLoader
from torchvision.datasets import MNIST
from torchvision import transforms
import pytorch_lightning as pl
from pytorch_lightning.callbacks import EarlyStopping

class LeNetModel(pl.LightningModule):

    def __init__(self):
        super(LeNetModel, self).__init__()
        self.abstract = torch.nn.Sequential(
            torch.nn.Conv2d(1, 6, 5, 1),
            torch.nn.Tanh(),
            torch.nn.AvgPool2d(2, 2),
            # torch.nn.Tanh(),
            torch.nn.Conv2d(6, 16, 5, 1),
            torch.nn.Tanh(),
            torch.nn.AvgPool2d(2, 2),
            # torch.nn.Tanh(),
            torch.nn.Conv2d(16, 120, 5, 1),
            torch.nn.Tanh(),
            torch.nn.Flatten()
        )
        self.fully_connected = torch.nn.Sequential(
            torch.nn.Linear(120, 84),
            torch.nn.Tanh(),
            torch.nn.Linear(84, 10),
            # torch.nn.Tanh()
        )

    def forward(self, x):
        logits = self.fully_connected(self.abstract(x))
        predictions = F.softmax(logits, dim=1)
        return predictions

    def training_step(self, batch, batch_nb):
        x, y = batch
        y_hat = self(x)
        loss = F.cross_entropy(y_hat, y)
        tensorboard_logs = {'train_loss': loss}
        return {'loss': loss, 'log': tensorboard_logs}

    def validation_step(self, batch, batch_nb):
        x, y = batch
        y_hat = self(x)
        loss = F.cross_entropy(y_hat, y)
        return {'val_loss': loss}

    def validation_epoch_end(self, outputs):
        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()
        tensorboard_logs = {'val_loss': avg_loss}
        return {'val_loss': avg_loss, 'log': tensorboard_logs}

    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=0.0001)

    def train_dataloader(self):
        transform_config = transforms.Compose(
            [
                transforms.Pad(2),
                transforms.ToTensor()
            ]
        )
        return DataLoader(MNIST(os.getcwd(), train=True, download=True, transform=transform_config), batch_size=32)

    def val_dataloader(self):
        transform_config = transforms.Compose(
            [
                transforms.Pad(2),
                transforms.ToTensor()
            ]
        )
        return DataLoader(MNIST(os.getcwd(), train=True, download=True, transform=transform_config), batch_size=32)

early_stop = EarlyStopping(
    monitor='val_loss',
    min_delta=0.00,
    patience=7,
    verbose=False,
    mode='auto'
)

lenet5_model = LeNetModel()
trainer = pl.Trainer(gpus=0, early_stop_callback=early_stop)
trainer.fit(lenet5_model)

def get_accuracy(model, data_loader, device):
    '''
    Function for computing the accuracy of the predictions over the entire data_loader
    '''
    correct_pred = 0
    n = 0
    with torch.no_grad():
        model.eval()
        for X, y_true in data_loader:
            X = X.to(device)
            y_true = y_true.to(device)
            y_prob = model(X)
            predicted_labels = torch.argmax(y_prob, 1)
            n += y_true.size(0)
            correct_pred += (predicted_labels == y_true).sum()
    return correct_pred.float() / n